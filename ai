package com.truist.cps.kafka.interceptor;

import com.truist.cps.kafka.config.KafkaSdkProperties;
import com.truist.cps.kafka.logging.SdkLogSanitizer;
import com.truist.cps.kafka.logging.SdkMdc;
import com.truist.cps.kafka.metrics.SdkListenerStateTracker;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.header.Header;
import org.apache.kafka.common.header.Headers;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.listener.RecordInterceptor;

import java.nio.charset.StandardCharsets;
import java.util.*;

/**
 * Intercepts records to apply consistent logging and MDC correlation.
 *
 * Responsibilities (MUST remain thin):
 * - Populate MDC correlation keys (traceId/correlationId/topic/partition/offset/groupId if available)
 * - Produce safe debug logs with header redaction (no payload dumps unless explicitly enabled)
 * - Increment lightweight "seen" counters via tracker (optional)
 *
 * Non-responsibilities (DO NOT add here):
 * - offset commits, pause/resume, retries, DLT publishing
 * - schema handling / deserialization logic
 * - network calls
 */
public final class SdkRecordInterceptor<K, V> implements RecordInterceptor<K, V> {

    private static final Logger log = LoggerFactory.getLogger(SdkRecordInterceptor.class);

    private final KafkaSdkProperties props;
    private final SdkListenerStateTracker tracker;

    public SdkRecordInterceptor(KafkaSdkProperties props,
                                SdkListenerStateTracker tracker) {
        this.props = Objects.requireNonNull(props, "props");
        this.tracker = Objects.requireNonNull(tracker, "tracker");
    }

    /**
     * Spring Kafka will call this before invoking @KafkaListener.
     * You MUST never throw from here; any exception would break consumption.
     */
    @Override
    public ConsumerRecord<K, V> intercept(ConsumerRecord<K, V> record) {
        if (record == null) {
            return null;
        }

        // Keep it fail-safe: do not let interceptor exceptions affect consumption.
        try {
            // 1) Build correlation info from headers (prefer) + fallback.
            final String correlationId = resolveCorrelationId(record.headers());
            final String traceId = resolveTraceId(record.headers());

            // 2) Apply MDC (always clear later)
            SdkMdc.put(SdkMdc.Keys.CORRELATION_ID, correlationId);
            SdkMdc.put(SdkMdc.Keys.TRACE_ID, traceId);
            SdkMdc.put(SdkMdc.Keys.TOPIC, record.topic());
            SdkMdc.put(SdkMdc.Keys.PARTITION, String.valueOf(record.partition()));
            SdkMdc.put(SdkMdc.Keys.OFFSET, String.valueOf(record.offset()));

            // Optional: groupId is not always available here (depends on your SDK wiring).
            // If you have it in properties, put it (safe).
            if (props.getConsumer() != null && props.getConsumer().getGroupId() != null) {
                SdkMdc.put(SdkMdc.Keys.GROUP_ID, props.getConsumer().getGroupId());
            }

            // 3) Lightweight metric hook (do not throw)
            tracker.onRecordSeen(record.topic(), record.partition());

            // 4) Debug logging (safe + redacted)
            if (props.getLogging() != null && props.getLogging().isDebugRecordLogEnabled() && log.isDebugEnabled()) {
                String headerSummary = buildSafeHeaderSummary(record.headers());
                log.debug("Kafka record received: topic={}, partition={}, offset={}, keyType={}, valueType={}, headers={}",
                        record.topic(),
                        record.partition(),
                        record.offset(),
                        record.key() == null ? "null" : record.key().getClass().getSimpleName(),
                        record.value() == null ? "null" : record.value().getClass().getSimpleName(),
                        headerSummary);
            }

            return record;
        } catch (Exception e) {
            // Never break the consumer pipeline because of interceptor bugs.
            log.warn("SdkRecordInterceptor failed (ignored). topic={}, partition={}, offset={}, err={}",
                    safe(record.topic()),
                    record.partition(),
                    record.offset(),
                    safe(e.toString()));
            return record;
        } finally {
            // IMPORTANT: always clear MDC to avoid leaking values across threads/records.
            SdkMdc.clear();
        }
    }

    // ----------------------------
    // Correlation ID / Trace ID
    // ----------------------------

    private String resolveCorrelationId(Headers headers) {
        KafkaSdkProperties.Correlation corr = props.getCorrelation();
        String headerKey = corr != null && corr.getCorrelationIdHeader() != null
                ? corr.getCorrelationIdHeader()
                : "correlationId";

        String value = firstHeaderAsString(headers, headerKey);
        if (value != null && !value.isBlank()) {
            return SdkLogSanitizer.sanitize(value);
        }

        // If missing, generate (optional)
        boolean generate = corr == null || corr.isGenerateIfMissing();
        if (generate) {
            return UUID.randomUUID().toString();
        }
        return "missing";
    }

    private String resolveTraceId(Headers headers) {
        KafkaSdkProperties.Correlation corr = props.getCorrelation();
        String headerKey = corr != null && corr.getTraceIdHeader() != null
                ? corr.getTraceIdHeader()
                : "traceId";

        String value = firstHeaderAsString(headers, headerKey);
        if (value != null && !value.isBlank()) {
            return SdkLogSanitizer.sanitize(value);
        }
        return "missing";
    }

    private String firstHeaderAsString(Headers headers, String key) {
        if (headers == null || key == null) return null;
        Header h = headers.lastHeader(key);
        if (h == null || h.value() == null) return null;
        return new String(h.value(), StandardCharsets.UTF_8);
    }

    // ----------------------------
    // Header summary + redaction
    // ----------------------------

    private String buildSafeHeaderSummary(Headers headers) {
        if (headers == null) {
            return "{}";
        }

        KafkaSdkProperties.Logging logging = props.getLogging();
        KafkaSdkProperties.HeaderRedaction redaction = (logging != null) ? logging.getHeaderRedaction() : null;

        int maxHeaders = (logging != null && logging.getMaxHeadersToLog() > 0)
                ? logging.getMaxHeadersToLog()
                : 30;

        int maxValueChars = (logging != null && logging.getMaxHeaderValueChars() > 0)
                ? logging.getMaxHeaderValueChars()
                : 128;

        Map<String, String> out = new LinkedHashMap<>();
        int count = 0;

        for (Header h : headers) {
            if (h == null) continue;
            if (count >= maxHeaders) {
                out.put("_truncated", "true");
                break;
            }

            String name = safe(h.key());
            String value = headerValueAsSafeString(h, maxValueChars);

            // Apply redaction policy
            if (shouldRedact(name, redaction)) {
                out.put(name, "***REDACTED***");
            } else {
                out.put(name, value);
            }
            count++;
        }

        return out.toString();
    }

    private boolean shouldRedact(String headerName, KafkaSdkProperties.HeaderRedaction redaction) {
        if (redaction == null) {
            // default safe behavior: redact common sensitive headers
            return isDefaultSensitiveHeader(headerName);
        }

        KafkaSdkProperties.HeaderRedaction.Mode mode = redaction.getMode();
        List<String> list = redaction.getKeys() == null ? List.of() : redaction.getKeys();
        String prefix = redaction.getPrefix();

        // Prefix-based redaction (e.g., "x-private-")
        if (prefix != null && !prefix.isBlank() && headerName != null) {
            if (headerName.toLowerCase(Locale.ROOT).startsWith(prefix.toLowerCase(Locale.ROOT))) {
                return true;
            }
        }

        // Mode-based allow/deny
        if (mode == KafkaSdkProperties.HeaderRedaction.Mode.WHITELIST) {
            // redact everything NOT explicitly listed
            return list.stream().noneMatch(k -> equalsIgnoreCase(k, headerName));
        } else if (mode == KafkaSdkProperties.HeaderRedaction.Mode.BLACKLIST) {
            // redact only listed items
            return list.stream().anyMatch(k -> equalsIgnoreCase(k, headerName)) || isDefaultSensitiveHeader(headerName);
        }

        // fallback
        return isDefaultSensitiveHeader(headerName);
    }

    private boolean isDefaultSensitiveHeader(String headerName) {
        if (headerName == null) return false;
        String h = headerName.toLowerCase(Locale.ROOT);
        return h.contains("authorization")
                || h.contains("token")
                || h.contains("secret")
                || h.contains("password")
                || h.contains("cookie")
                || h.contains("set-cookie")
                || h.contains("ssn");
    }

    private String headerValueAsSafeString(Header h, int maxChars) {
        if (h == null || h.value() == null) {
            return "null";
        }
        String s = new String(h.value(), StandardCharsets.UTF_8);
        s = SdkLogSanitizer.sanitize(s);
        if (s.length() > maxChars) {
            return s.substring(0, maxChars) + "...";
        }
        return s;
    }

    private boolean equalsIgnoreCase(String a, String b) {
        if (a == null || b == null) return false;
        return a.equalsIgnoreCase(b);
    }

    private String safe(String s) {
        return s == null ? "null" : SdkLogSanitizer.sanitize(s);
    }
}